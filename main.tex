\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2cm,bmargin=2cm,lmargin=2cm,rmargin=2cm}
\usepackage{color}
\usepackage{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{parskip}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},backref=page,colorlinks=true]
 {hyperref}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\usepackage[natbibapa]{apacite}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{pdfsync}
\usepackage{showlabels}

\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\st}{s.t.}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Ne}{Ne}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\vvec}{vec}
\DeclareMathOperator{\pa}{pa}
\DeclareMathOperator{\repmat}{repmat}

\def\ci{\perp\!\!\!\perp}

\makeatother

\begin{document}

\title{Expanding Compressed Learning}

\author{Kevin Shi and Kui Tang}
\maketitle
\begin{abstract}
Compressed sensing can offer fast and simple techniques for machine learning in high dimensional space. Recently, \citet{Calderbank09} have shown that the RIP$_2$ property of a measurement matrix allows us to train linear support vector machines on compressed measurements directly, rather than recovering a high dimensional sparse vector and working in that space.
They bound the difference of the error of a classifier trained on low-dimensional samples vs. the error of the best classifier on the high-dimensional space by a factor that depends on the data radius, regularization, and the RIP $\epsilon$ parameter.
We show that nearly the same bounds hold for support vector regression (i.e. continuous labels), and for nonlinear models using the squared exponential kernel, though by an exponential factor.
Next we explore a new nonnegative randomized RIP construction using Poisson random variables.
This offers an alternative to the standard Gaussian and Bernoulli RIP matrices, and may be step in the direction towards constructing expander graph RIP matrices.
\end{abstract}

\section{Introduction}

Compressed sensing is based off of two fundamental ideas.
First is constructing an $m \times n$ \emph{measurement matrix} $A$ which satisfies $\emph{restricted isometry property}$ in $L_p$ with parameters $(k,\epsilon)$, abbreviated $\mbox{RIP}_p(k,\epsilon)$.
This means that for any $k-$sparse $x$, $$(1-\epsilon)\|x\|_p \le\|Ax\|_p \le (1+\epsilon)\|x\|_p.$$
Well-known examples are matrices whose entries are Gaussian $\mathcal{N}(0,\sigma=1/\sqrt{m})$ and ones whose entries are Bernoulli $U(-1/\sqrt{m},1/\sqrt{m})$~\citep{Baraniuk08}.
But these classical constructions have a caveat: being random matrices, the RIP only holds with high probability.
An open question whether we can deterministically construct measurement matrices with RIP.

The next idea is the compressed sensing linear program~\citep{Candes06}.
Given an input measurement $x$ and a measurement matrix $A$, we seek the solution to
\begin{alignat*}{1}
\min & \left\Vert \widehat{x}\right\Vert _{1}\\
\text{s.t.} & A\widehat{x}=x
\end{alignat*}
Let $y$ be the output of this linear program.
If $A$ satisfying the RIP$_2$ property, then the guarantee is that for any $k$-sparse vector $\tilde{x}$, we have $\|y - x\|_2 \le O(1/\sqrt{k}) \|\tilde{x}-x\|_1$.
In particular this holds for the best $k-$sparse approximation, that is, $\arg\min_{\tilde{x}} \|\tilde{x}-x\|_1$, so this is a rather strong agnostic learning guarantee.
Naturally, we may ask whether the RIP can help use solve learning problems beyond that of $L_1$ signal recovery.

\section{Compressed learning}
\citet{Calderbank09} show it is possible to merge compressive sensing and learning. That is, if our true data are $k$-sparse vectors and we observe compressively-sensed samples and train an SVM directly on these low-dimensional samples, we obtain a model whose generalization error is comparable to that of the best classifier over the true directly on the true data. Their argument first shows that if the measurement matrix $A$ is RIP$_2$, then inner products between convex hulls of $k$-sparse vectors also satisfy a sort of RIP property that also depends on the radius of the data. They then apply this result to compare the \emph{projection} of a classifier learned over the true data points, $A\widehat{w}_S$ to the classifier learned over the compressed samples, $\widehat{w}_{AS}$. In particular, they show that 
\begin{equation}L_{\mathcal{D}}(A \widehat{w}_S) \leq L_{\mathcal{D}}(\widehat{w}_S) + O(CR^2\epsilon)
\label{eq:projection-loss-bound}
\end{equation}
where $R$ is the radius of the data, $C$ is a regularization constant, and $L_{\mathcal{D}}$ is the empirical regularization loss. This result can be combined with standard results on generalization of SVMs to obtain a bound 
\begin{equation}
H_{\mathcal{D}}(\widehat{z}_{AS}) \leq H_{\mathcal{D}}(w_0) + O\left( \sqrt{\left\Vert w_0 \right\Vert^2 \left( R^2 \epsilon + \frac{\log(1/\delta)}{M} \right)} \right)
\end{equation}
where $H_\mathcal{D}$ is the empirical hinge loss and $w_0$ is the best classifier in the data domain (full-dimensional space).

We plan to explore extensions of this idea:
\begin{enumerate}
\item Expand the RIP property of dot products to nonlinear kernels. Preliminary calculations show a similar property holds for squared-exponential kernels, albeit with a factor of $\exp(-2\epsilon R - \epsilon^2 R)$ for the lower bound and $\exp(2 \epsilon R)$ for the upper bound. Bounds on polynomial kernels also seem possible by analyzing eigenvalues of the Gram matrix.
\item Generalize compressive learning to other methods, such as kernel generalized linear models (GLMs). This involves showing an RIP property holds for \emph{linear} combinations support vectors. The current work only proves this for convex combinations because they suffice for SVMs, but kernel GLMs use linear combinations of support vectors. This can be done by performing a more careful analysis. We should be able to rely on standard results to show an analogue of~\eqref{eq:projection-loss-bound}: that the \emph{log-likelihoods} of the projected  model $A\widehat{w}_S$ and the model trained on compressed samples $\widehat{w}_{AS}$ are close, because the log-likelihood is linear in the parameters $w$. We will need to explore the literature further to find analogues of the generalization guarantees for SVMs that apply to bounds we can derive here.
\end{enumerate}

\section{RIP$_2$ from expander graphs}

The $\ell_2$ norm is the most interesting norm for machine learning applications, because many models are linear models in an appropriate feature space. However the expander graph construction from \citet{Berinde2008} only constructs an RIP$_p$ matrix for $1 \le p < 1+O(1/\log n)$. For the $\ell_2$-norm, there is a lower bound such that for any $A \in \{0,1\}^{m\times n}$ satisfies the RIP$_2$ property, it must be that $m = \Omega(k^2)$. To our knowledge there is no construction matching this lower bound. 

One particular application of such an expander graph, should we be able to find one, is to extend the ideas of the previous section to compressive learning with a sparse expander graph as the measurement matrix, rather than a dense complete matrix as presently must be the case. This would give us a learning algorithm requiring fewer arithmetic operations.

\section{Derandomized data-dependent dimensionality reduction}

Compressed sensing seems to be a more powerful form of sketching in that instead of mapping data points to $k$ fixed bins, these data points can be mapped to potentially different subsets of $k$ coordinates within the entire $n$-dimensional space yet still retain a sensible metric between two embeddings into different $k$-subsets. In particular we propose the sketch function $H_A(x)$ which computes $\tilde{x} = \arg\min_{\hat{x}} \|\hat{x}\|_1$ such that $A\hat{x}=x$ and then keeps only the top $k$ entries of $\tilde{x}$, setting the rest to $0$. The goal is that, given a data set $X$ in advance, we construct a matrix $A$ such that every vector $x$ is close to $A\tilde{x}$ for some sparse vector $\tilde{x}$. The linear program is then computing an approximation of $\tilde{x}$, and we have the agnostic learning guarantee that this is a good $\left(O(1/\sqrt{k})\right)$ approximation. 

\bibliographystyle{plainnat}
\bibliography{biblio}

\end{document}
