%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Kui Tang at 2015-12-12 19:31:54 -0500 


%% Saved with string encoding Unicode (UTF-8) 



@article{Shaffer1991,
	Abstract = {In the standard linear regression model with independent, homoscedastic errors, the Gauss-Markov theorem asserts that Œ≤ÃÇ, = (X'X)-1(X'y) is the best linear unbiased estimator of Œ≤ and, furthermore, that c'Œ≤ÃÇ is the best linear unbiased estimator of c'Œ≤ for all p √ó 1 vectors c. In the corresponding random regressor model, X is a random sample of size n from a p-variate distribution. If attention is restricted to linear estimators of c'Œ≤ that are conditionally unbiased, given X, the Gauss-Markov theorem applies. If, however, the estimator is required only to be unconditionally unbiased, the Gauss-Markov theorem may or may not hold, depending on what is known about the distribution of X. The results generalize to the case in which X is a random sample without replacement from a finite population.},
	Author = {Juliet Popper Shaffer},
	Date-Added = {2015-12-13 00:31:51 +0000},
	Date-Modified = {2015-12-13 00:31:53 +0000},
	Issn = {00031305},
	Journal = {The American Statistician},
	Number = {4},
	Pages = {269-273},
	Publisher = {Taylor & Francis, Ltd.},
	Title = {The Gauss-Markov Theorem and Random Regressors},
	Url = {http://www.jstor.org/stable/2684451},
	Volume = {45},
	Year = {1991},
	Bdsk-Url-1 = {http://www.jstor.org/stable/2684451}}

@article{Candes2006,
	Abstract = {Suppose we wish to recover a vector x0 ‚àà ‚ÑùùìÇ (e.g., a digital signal or image) from incomplete and contaminated observations y = A x0 + e; A is an ùìÉ √ó ùìÇ matrix with far fewer rows than columns (ùìÉ ‚â™ ùìÇ) and e is an error term. Is it possible to recover x0 accurately based on the data y?To recover x0, we consider the solution x# to the ùìÅ1-regularization problem $${\rm \min}\; |x\|_{\ell_1} \quad {\rm subject \; to } \; \|Ax-y\|_{\ell_2} \leq \epsilon,$$ where œµ is the size of the error term e. We show that if A obeys a uniform uncertainty principle (with unit-normed columns) and if the vector x0 is sufficiently sparse, then the solution is within the noise level $$\|x^\sharp - x_0\|_{\ell_2} \leq C \cdot \epsilon.$$As a first example, suppose that A is a Gaussian random matrix; then stable recovery occurs for almost all such A's provided that the number of nonzeros of x0 is of about the same order as the number of observations. As a second instance, suppose one observes few Fourier samples of x0; then stable recovery occurs for almost any set of ùìÉ coefficients provided that the number of nonzeros is of the order of ùìÉ/(log ùìÇ)6.In the case where the error term vanishes, the recovery is of course exact, and this work actually provides novel insights into the exact recovery phenomenon discussed in earlier papers. The methodology also explains why one can also very nearly recover approximately sparse signals. {\copyright} 2006 Wiley Periodicals, Inc.},
	Author = {Cand{\`e}s, Emmanuel J. and Romberg, Justin K. and Tao, Terence},
	Date-Added = {2015-12-12 19:48:41 +0000},
	Date-Modified = {2015-12-12 19:48:42 +0000},
	Doi = {10.1002/cpa.20124},
	Issn = {1097-0312},
	Journal = {Communications on Pure and Applied Mathematics},
	Number = {8},
	Pages = {1207--1223},
	Publisher = {Wiley Subscription Services, Inc., A Wiley Company},
	Title = {Stable signal recovery from incomplete and inaccurate measurements},
	Url = {http://dx.doi.org/10.1002/cpa.20124},
	Volume = {59},
	Year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1002/cpa.20124}}

@article{Baraniuk08,
	Author = {Baraniuk, Richard and Davenport, Mark and DeVore, Ronald and Wakin, Michael},
	Date-Added = {2015-12-12 19:44:29 +0000},
	Date-Modified = {2015-12-12 19:44:29 +0000},
	Doi = {10.1007/s00365-007-9003-x},
	Issn = {0176-4276},
	Journal = {Constructive Approximation},
	Keywords = {Compressed sensing; Sampling; Random matrices; Concentration inequalities; 15N2; 15A52; 60F10; 94A12; 94A20},
	Language = {English},
	Number = {3},
	Pages = {253-263},
	Publisher = {Springer-Verlag},
	Title = {A Simple Proof of the Restricted Isometry Property for Random Matrices},
	Url = {http://dx.doi.org/10.1007/s00365-007-9003-x},
	Volume = {28},
	Year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s00365-007-9003-x}}

@techreport{Sridharan08,
	Author = {Karthik Sridharan and Nathan Srebro and Shai Shalev-Shwartz},
	Date-Added = {2015-12-11 00:52:18 +0000},
	Date-Modified = {2015-12-11 00:53:04 +0000},
	Institution = {TTIC},
	Title = {Fast Convergence Rates for Excess Regularized Risk with Application to SVM},
	Year = {2008}}

@article{Hoory2006,
	Author = {Hoory, Shlomo and Linial, Nathan and Wigderson, Avi},
	Coden = {BAMOAD},
	Date-Added = {2015-11-16 22:56:09 +0000},
	Date-Modified = {2015-11-16 22:56:10 +0000},
	Doi = {10.1090/S0273-0979-06-01126-8},
	Fjournal = {American Mathematical Society. Bulletin. New Series},
	Issn = {0273-0979},
	Journal = {Bull. Amer. Math. Soc. (N.S.)},
	Mrclass = {68Q15 (00-02 05C25 05C80 60G50 68Q17 68R10)},
	Mrnumber = {2247919 (2007h:68055)},
	Mrreviewer = {Mark R. Jerrum},
	Number = {4},
	Pages = {439--561 (electronic)},
	Title = {Expander graphs and their applications},
	Url = {http://dx.doi.org/10.1090/S0273-0979-06-01126-8},
	Volume = {43},
	Year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1090/S0273-0979-06-01126-8}}

@techreport{Calderbank09,
	Author = {Robert Calderbank and Sina Jafarpour and Robert Schapire},
	Date-Added = {2015-11-16 22:53:23 +0000},
	Date-Modified = {2015-11-16 22:54:00 +0000},
	Institution = {Princeton University},
	Title = {Compressed learning: Universal sparse dimensionality reduction and learning in the measurement domain},
	Year = {2009}}

@article{Bandeira12,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2012arXiv1204.1580B},
	Archiveprefix = {arXiv},
	Author = {{Bandeira}, A.~S. and {Dobriban}, E. and {Mixon}, D.~G. and {Sawin}, W.~F.},
	Eprint = {1204.1580},
	Journal = {ArXiv e-prints},
	Keywords = {Mathematics - Functional Analysis, Computer Science - Computational Complexity, Computer Science - Information Theory},
	Month = apr,
	Primaryclass = {math.FA},
	Title = {{Certifying the restricted isometry property is hard}},
	Year = 2012}

@inproceedings{Berinde2008,
	Abstract = {There are two main algorithmic approaches to sparse signal recovery: geometric and combinatorial. The geometric approach utilizes geometric properties of the measurement matrix Phi. A notable example is the Restricted Isometry Property, which states that the mapping Phi preserves the Euclidean norm of sparse signals; it is known that random dense matrices satisfy this constraint with high probability. On the other hand, the combinatorial approach utilizes sparse matrices, interpreted as adjacency matrices of sparse (possibly random) graphs, and uses combinatorial techniques to recover an approximation to the signal. In this paper we present a unification of these two approaches. To this end, we extend the notion of Restricted Isometry Property from the Euclidean lscr2 norm to the Manhattan lscr1 norm. Then we show that this new lscr1 -based property is essentially equivalent to the combinatorial notion of expansion of the sparse graph underlying the measurement matrix. At the same time we show that the new property suffices to guarantee correctness of both geometric and combinatorial recovery algorithms. As a result, we obtain new measurement matrix constructions and algorithms for signal recovery which, compared to previous algorithms, are superior in either the number of measurements or computational efficiency of decoders.},
	Author = {Berinde, R. and Gilbert, A.C. and Indyk, P. and Karloff, H. and Strauss, M.J.},
	Booktitle = {Communication, Control, and Computing, 2008 46th Annual Allerton Conference on},
	Date-Added = {2015-11-16 22:50:31 +0000},
	Date-Modified = {2015-11-16 22:50:31 +0000},
	Doi = {10.1109/ALLERTON.2008.4797639},
	Keywords = {geometry;matrix algebra;signal processing;Euclidean norm;combinatorial approach;combinatorics;geometry;restricted isometry property;signal approximation;sparse signal recovery;sparse signals;Approximation error;Cameras;Combinatorial mathematics;Computer networks;Geometry;Image coding;Signal processing;Signal processing algorithms;Sparse matrices;Vectors},
	Month = {Sept},
	Pages = {798-805},
	Title = {Combining geometry and combinatorics: A unified approach to sparse signal recovery},
	Year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ALLERTON.2008.4797639}}
